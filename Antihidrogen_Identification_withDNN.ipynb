{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OxgbNo79MnrW"
      },
      "source": [
        "# PROGETTO: ANNICHILIMENTO DELL'ANTIIDROGENO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dBNaqcuM092"
      },
      "source": [
        "# 1) Gestione dati"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NOAKjtrDK5zo",
        "outputId": "db9c0c73-10e1-4875-b91f-3e0d85364661"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-07-09 13:04:40--  http://mlphysics.ics.uci.edu/data/antihydrogen/antihydrogen_test.csv.gz\n",
            "Resolving mlphysics.ics.uci.edu (mlphysics.ics.uci.edu)... 128.195.1.86\n",
            "Connecting to mlphysics.ics.uci.edu (mlphysics.ics.uci.edu)|128.195.1.86|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6608724 (6.3M) [application/x-gzip]\n",
            "Saving to: ‘antihydrogen_test.csv.gz’\n",
            "\n",
            "antihydrogen_test.c 100%[===================>]   6.30M  3.65MB/s    in 1.7s    \n",
            "\n",
            "2022-07-09 13:04:43 (3.65 MB/s) - ‘antihydrogen_test.csv.gz’ saved [6608724/6608724]\n",
            "\n",
            "--2022-07-09 13:04:43--  http://mlphysics.ics.uci.edu/data/antihydrogen/antihydrogen_train.csv.gz\n",
            "Resolving mlphysics.ics.uci.edu (mlphysics.ics.uci.edu)... 128.195.1.86\n",
            "Connecting to mlphysics.ics.uci.edu (mlphysics.ics.uci.edu)|128.195.1.86|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 19840043 (19M) [application/x-gzip]\n",
            "Saving to: ‘antihydrogen_train.csv.gz’\n",
            "\n",
            "antihydrogen_train. 100%[===================>]  18.92M  7.92MB/s    in 2.4s    \n",
            "\n",
            "2022-07-09 13:04:46 (7.92 MB/s) - ‘antihydrogen_train.csv.gz’ saved [19840043/19840043]\n",
            "\n",
            "--2022-07-09 13:04:46--  http://mlphysics.ics.uci.edu/data/antihydrogen/antihydrogen_valid.csv.gz\n",
            "Resolving mlphysics.ics.uci.edu (mlphysics.ics.uci.edu)... 128.195.1.86\n",
            "Connecting to mlphysics.ics.uci.edu (mlphysics.ics.uci.edu)|128.195.1.86|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6622198 (6.3M) [application/x-gzip]\n",
            "Saving to: ‘antihydrogen_valid.csv.gz’\n",
            "\n",
            "antihydrogen_valid. 100%[===================>]   6.31M  3.96MB/s    in 1.6s    \n",
            "\n",
            "2022-07-09 13:04:48 (3.96 MB/s) - ‘antihydrogen_valid.csv.gz’ saved [6622198/6622198]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# IMPORT DATI\n",
        "!rm -rf antihydrogen_test.csv\n",
        "!rm -rf antihydrogen_valid.csv\n",
        "!rm -rf antihydrogen_train.csv\n",
        "\n",
        "!wget http://mlphysics.ics.uci.edu/data/antihydrogen/antihydrogen_test.csv.gz\n",
        "!wget http://mlphysics.ics.uci.edu/data/antihydrogen/antihydrogen_train.csv.gz\n",
        "!wget http://mlphysics.ics.uci.edu/data/antihydrogen/antihydrogen_valid.csv.gz\n",
        "\n",
        "!gzip -d antihydrogen_test.csv.gz\n",
        "!gzip -d antihydrogen_valid.csv.gz\n",
        "!gzip -d antihydrogen_train.csv.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E7Do9WZ-wUqX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "142085d4-9bd5-4a2b-9682-7b28ab4daaed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.11.0+cu113\n",
            "0.12.0+cu113\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import time\n",
        "\n",
        "from torchsummary import summary\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "print(torch.__version__)\n",
        "print(torchvision.__version__)\n",
        "\n",
        "from torch import nn, optim\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms, models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8a3cLrC3wWBt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "545fe539-d287-48f1-89ac-61cabcde464b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Numero di GPU disponibili:  1\n",
            "Tesla T4\n",
            "Computation device: cuda\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# check if GPU is avaible and which kind\n",
        "if torch.cuda.is_available():\n",
        "  print('Numero di GPU disponibili: ',torch.cuda.device_count())\n",
        "  for i in range(0,torch.cuda.device_count()):\n",
        "    print(torch.cuda.get_device_name(0))\n",
        "\n",
        "# if GPU is avaible: device='cuda', otherwise 'cpu'\n",
        "device = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Computation device: {device}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TQGFcb-pMwcs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d33ce88d-60e2-4307-98d8-8677b2db1d96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tempo impiegato: 0.3 min\n"
          ]
        }
      ],
      "source": [
        "#Data analysis\n",
        "\n",
        "#Column 0: Target\n",
        "#Columns 1-447: Inner-Z, 447 features.\n",
        "#Columns 448-693: Inner-PHI, 246 features.\n",
        "#Columns 694-1140: Outer-Z, 447 features.\n",
        "#Columns 1141-1430: Outer-PHI, 290 features.\n",
        "\n",
        "Neve = 200000\n",
        "t0 = time.time()\n",
        "#Read only Neve of the total events, leaving unchanged the ratio train/test/vali : 60/20/20\n",
        "#In questo modo non si hanno problemi di RAM\n",
        "tr_df = pd.read_csv('antihydrogen_train.csv', dtype=np.int8, sep=' ', nrows = Neve) \n",
        "val_df = pd.read_csv('antihydrogen_valid.csv', dtype=np.int8, sep=' ', nrows = int(Neve * 0.3))\n",
        "tes_df = pd.read_csv('antihydrogen_test.csv', dtype=np.int8, sep=' ', nrows = int(Neve*0.3))\n",
        "\n",
        "print(\"tempo impiegato:\", round((time.time()-t0)/60, 1), \"min\")\n",
        "#time to read all the events: 2.6 min\n",
        "#time to read 200k: 0.6 min"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RYsc-6pReD1V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e68b38f-81b9-4782-c838-17cb4f85921d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of datas training: 200000\n",
            "# Number of datas validation: 60000\n",
            "# Number of datas test: 60000\n"
          ]
        }
      ],
      "source": [
        "N_train = len(tr_df.index)\n",
        "N_vali = len(val_df.index)\n",
        "N_test = len(tes_df.index)\n",
        "\n",
        "print(\"Number of datas training:\", N_train)\n",
        "print(\"# Number of datas validation:\", N_vali)\n",
        "print(\"# Number of datas test:\", N_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OUUK-V6VB4ze",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "9a1b5a5b-2485-4893-e60c-37ed0bddd25e"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "   Target  Inner-Z1  Inner-Z2  Inner-Z3  Inner-Z4  Inner-Z5  Inner-Z6  \\\n",
              "0       0         0         0         0         0         0         0   \n",
              "1       0         0         0         0         0         0         0   \n",
              "2       1         0         0         0         0         0         0   \n",
              "3       0         0         0         0         0         0         0   \n",
              "4       1         0         0         0         0         0         0   \n",
              "\n",
              "   Inner-Z7  Inner-Z8  Inner-Z9  ...  Outer-Phi281  Outer-Phi282  \\\n",
              "0         0         0         0  ...             0             0   \n",
              "1         0         0         0  ...             0             0   \n",
              "2         0         0         0  ...             0             0   \n",
              "3         0         0         0  ...             0             0   \n",
              "4         0         0         0  ...             0             0   \n",
              "\n",
              "   Outer-Phi283  Outer-Phi284  Outer-Phi285  Outer-Phi286  Outer-Phi287  \\\n",
              "0             0             0             0             0             0   \n",
              "1             0             0             0             0             0   \n",
              "2             0             0             0             0             0   \n",
              "3             0             0             0             0             0   \n",
              "4             0             0             0             0             0   \n",
              "\n",
              "   Outer-Phi288  Outer-Phi289  Outer-Phi290  \n",
              "0             0             0             0  \n",
              "1             0             0             0  \n",
              "2             0             0             0  \n",
              "3             0             0             0  \n",
              "4             0             0             0  \n",
              "\n",
              "[5 rows x 1431 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3a552ce0-49d9-47af-9129-0e1d914049af\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Target</th>\n",
              "      <th>Inner-Z1</th>\n",
              "      <th>Inner-Z2</th>\n",
              "      <th>Inner-Z3</th>\n",
              "      <th>Inner-Z4</th>\n",
              "      <th>Inner-Z5</th>\n",
              "      <th>Inner-Z6</th>\n",
              "      <th>Inner-Z7</th>\n",
              "      <th>Inner-Z8</th>\n",
              "      <th>Inner-Z9</th>\n",
              "      <th>...</th>\n",
              "      <th>Outer-Phi281</th>\n",
              "      <th>Outer-Phi282</th>\n",
              "      <th>Outer-Phi283</th>\n",
              "      <th>Outer-Phi284</th>\n",
              "      <th>Outer-Phi285</th>\n",
              "      <th>Outer-Phi286</th>\n",
              "      <th>Outer-Phi287</th>\n",
              "      <th>Outer-Phi288</th>\n",
              "      <th>Outer-Phi289</th>\n",
              "      <th>Outer-Phi290</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 1431 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3a552ce0-49d9-47af-9129-0e1d914049af')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3a552ce0-49d9-47af-9129-0e1d914049af button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3a552ce0-49d9-47af-9129-0e1d914049af');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# CREATE NEW COLUMNS FOR THE DF\n",
        "c0_label = 'Target'\n",
        "c0_idx = 0\n",
        "c1_label = 'Inner-Z'\n",
        "c1_idx_min = c0_idx + 1\n",
        "c1_idx_max = 447\n",
        "c2_label = 'Inner-Phi'\n",
        "c2_idx_min = c1_idx_max + 1\n",
        "c2_idx_max = 693\n",
        "c3_label = 'Outer-Z'\n",
        "c3_idx_min = c2_idx_max + 1\n",
        "c3_idx_max = 1140\n",
        "c4_label = 'Outer-Phi'\n",
        "c4_idx_min = c3_idx_max + 1\n",
        "c4_idx_max = 1430\n",
        "\n",
        "colonne = []\n",
        "colonne.append(c0_label)\n",
        "c1 = [c1_label + str(i+1) for i in range(0, c1_idx_max - c1_idx_min+1)]\n",
        "colonne += c1\n",
        "c2 = [c2_label + str(i+1) for i in range(0, c2_idx_max - c2_idx_min+1)]\n",
        "colonne += c2\n",
        "c3 = [c3_label + str(i+1) for i in range(0, c3_idx_max - c3_idx_min+1)]\n",
        "colonne += c3\n",
        "c4 = [c4_label + str(i+1) for i in range(0, c4_idx_max - c4_idx_min+1)]\n",
        "colonne += c4\n",
        "\n",
        "# SWITCH COLUMNS NAME\n",
        "tr_df.columns = colonne\n",
        "val_df.columns = colonne\n",
        "tes_df.columns = colonne\n",
        "\n",
        "# SHOW DF\n",
        "display(tr_df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LWkGu9twTZx_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "146101eb-97ea-4744-d264-bc44970a7ed3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Numero eventi di segnale train 98589.\n",
            "Numero eventi di fondo train 101411.\n",
            "\n",
            "\n",
            "Numero eventi di segnale test 29415.\n",
            "Numero eventi di fondo test 30585.\n",
            "\n",
            "\n",
            "Numero eventi di segnale vali 29491.\n",
            "Numero eventi di fondo vali 30509.\n"
          ]
        }
      ],
      "source": [
        "# select only the events with label 1\n",
        "tr_signal = tr_df.loc[tr_df['Target']>0.5]\n",
        "# select only the events with label 0\n",
        "tr_background = tr_df.loc[tr_df['Target']<0.5]\n",
        "\n",
        "\n",
        "# select only the events with label 1\n",
        "tes_signal = tes_df.loc[tes_df['Target']>0.5]\n",
        "# select only the events with label 0\n",
        "tes_background = tes_df.loc[tes_df['Target']<0.5]\n",
        "\n",
        "# select only the events with label 1\n",
        "val_signal = val_df.loc[val_df['Target']>0.5]\n",
        "# select only the events with label 0\n",
        "val_background = val_df.loc[val_df['Target']<0.5]\n",
        "\n",
        "print(f'Numero eventi di segnale train {len(tr_signal)}.')\n",
        "print(f'Numero eventi di fondo train {len(tr_background)}.')\n",
        "print('\\n')\n",
        "\n",
        "print(f'Numero eventi di segnale test {len(tes_signal)}.')\n",
        "print(f'Numero eventi di fondo test {len(tes_background)}.')\n",
        "print('\\n')\n",
        "\n",
        "print(f'Numero eventi di segnale vali {len(val_signal)}.')\n",
        "print(f'Numero eventi di fondo vali {len(val_background)}.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxBIebjA_yaN"
      },
      "source": [
        "Slight imbalance in favor of signal events. Given the large number of data, we can undersample by removing random rows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bVHh0Ftg-TMP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8a68a43-9997-4665-8105-c677fe111295"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Numero eventi di segnale train 98589.\n",
            "Numero eventi di fondo train 98589.\n",
            "\n",
            "\n",
            "Numero eventi di segnale test 29415.\n",
            "Numero eventi di fondo test 29415.\n",
            "\n",
            "\n",
            "Numero eventi di segnale vali 29491.\n",
            "Numero eventi di fondo vali 29491.\n"
          ]
        }
      ],
      "source": [
        "tr_df = tr_df.drop(tr_df[tr_df.Target.eq(0)].sample(np.abs(len(tr_signal)-len(tr_background))).index)\n",
        "tes_df = tes_df.drop(tes_df[tes_df.Target.eq(0)].sample(np.abs(len(tes_signal)-len(tes_background))).index)\n",
        "val_df = val_df.drop(val_df[val_df.Target.eq(0)].sample(np.abs(len(val_signal)-len(val_background))).index)\n",
        "\n",
        "# select only the events with label 1\n",
        "tr_signal = tr_df.loc[tr_df['Target']>0.5]\n",
        "# select only the events with label 0\n",
        "tr_background = tr_df.loc[tr_df['Target']<0.5]\n",
        "\n",
        "\n",
        "# select only the events with label 1\n",
        "tes_signal = tes_df.loc[tes_df['Target']>0.5]\n",
        "# select only the events with label 0\n",
        "tes_background = tes_df.loc[tes_df['Target']<0.5]\n",
        "\n",
        "# select only the events with label 1\n",
        "val_signal = val_df.loc[val_df['Target']>0.5]\n",
        "# select only the events with label 0\n",
        "val_background = val_df.loc[val_df['Target']<0.5]\n",
        "\n",
        "print(f'Numero eventi di segnale train {len(tr_signal)}.')\n",
        "print(f'Numero eventi di fondo train {len(tr_background)}.')\n",
        "print('\\n')\n",
        "\n",
        "print(f'Numero eventi di segnale test {len(tes_signal)}.')\n",
        "print(f'Numero eventi di fondo test {len(tes_background)}.')\n",
        "print('\\n')\n",
        "\n",
        "print(f'Numero eventi di segnale vali {len(val_signal)}.')\n",
        "print(f'Numero eventi di fondo vali {len(val_background)}.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RAjH3n-__xA"
      },
      "source": [
        "Perfectly balanced"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g3ZIlJIlGnxF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "acb3241a-8c24-4811-d700-019de4feeddd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape tr_target: (197178,)\n",
            "shape tr_innerZ: (197178, 447)\n",
            "shape tr_innerPhi: (197178, 246)\n",
            "shape tr_outerZ: (197178, 447)\n",
            "shape tr_outerPhi: (197178, 290)\n",
            "\n",
            "\n",
            "shape tes_target: (58830,)\n",
            "shape tes_innerZ: (58830, 447)\n",
            "shape tes_innerPhi: (58830, 246)\n",
            "shape tes_outerZ: (58830, 447)\n",
            "shape tes_outerPhi: (58830, 290)\n",
            "\n",
            "\n",
            "shape val_target: (58982,)\n",
            "shape val_innerZ: (58982, 447)\n",
            "shape val_innerPhi: (58982, 246)\n",
            "shape val_outerZ: (58982, 447)\n",
            "shape val_outerPhi: (58982, 290)\n"
          ]
        }
      ],
      "source": [
        "# DIVIDE THE DATAS IN 5 CATEGORIES: TARGET, INNER-Z, INNER-PHI, OUTER-Z, OUTER-PHI\n",
        "tr_target = np.array(tr_df[c0_label])\n",
        "tr_innerZ = np.array(tr_df[c1])\n",
        "tr_innerPhi = np.array(tr_df[c2])\n",
        "tr_outerZ = np.array(tr_df[c3])\n",
        "tr_outerPhi = np.array(tr_df[c4])\n",
        "\n",
        "tes_target = np.array(tes_df[c0_label])\n",
        "tes_innerZ = np.array(tes_df[c1])\n",
        "tes_innerPhi = np.array(tes_df[c2])\n",
        "tes_outerZ = np.array(tes_df[c3])\n",
        "tes_outerPhi = np.array(tes_df[c4])\n",
        "\n",
        "val_target = np.array(val_df[c0_label])\n",
        "val_innerZ = np.array(val_df[c1])\n",
        "val_innerPhi = np.array(val_df[c2])\n",
        "val_outerZ = np.array(val_df[c3])\n",
        "val_outerPhi = np.array(val_df[c4])\n",
        "\n",
        "print(\"shape tr_target:\", tr_target.shape)\n",
        "print(\"shape tr_innerZ:\", tr_innerZ.shape)\n",
        "print(\"shape tr_innerPhi:\", tr_innerPhi.shape)\n",
        "print(\"shape tr_outerZ:\", tr_outerZ.shape)\n",
        "print(\"shape tr_outerPhi:\", tr_outerPhi.shape)\n",
        "print('\\n')\n",
        "\n",
        "print(\"shape tes_target:\", tes_target.shape)\n",
        "print(\"shape tes_innerZ:\", tes_innerZ.shape)\n",
        "print(\"shape tes_innerPhi:\", tes_innerPhi.shape)\n",
        "print(\"shape tes_outerZ:\", tes_outerZ.shape)\n",
        "print(\"shape tes_outerPhi:\", tes_outerPhi.shape)\n",
        "print('\\n')\n",
        "\n",
        "print(\"shape val_target:\", val_target.shape)\n",
        "print(\"shape val_innerZ:\", val_innerZ.shape)\n",
        "print(\"shape val_innerPhi:\", val_innerPhi.shape)\n",
        "print(\"shape val_outerZ:\", val_outerZ.shape)\n",
        "print(\"shape val_outerPhi:\", val_outerPhi.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4S6l0NlgBHOJ"
      },
      "outputs": [],
      "source": [
        "# Features ranking, we want that tr_outerPhi and tr_innerPhi have the same shape\n",
        "\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "\n",
        "\n",
        "forest = ExtraTreesClassifier(n_estimators=100,\n",
        "                              random_state=0)\n",
        "\n",
        "\n",
        "forest.fit(tr_outerPhi, tr_target)\n",
        "importances = forest.feature_importances_\n",
        "std = np.std([tree.feature_importances_ for tree in forest.estimators_],\n",
        "            axis=0)\n",
        "indices = np.argsort(importances)[::-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BgFOaUfVEaxa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "691ef9e0-9d09-4050-e029-9f7aa8264f09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tr_outerPhi shape: (197178, 246)\n",
            "tes_outerPhi shape: (58830, 246)\n",
            "val_outerPhi shape: (58982, 246)\n"
          ]
        }
      ],
      "source": [
        "features = list(tr_df.columns)\n",
        "outerPhi_imp = np.array([features[j+1140] for j in indices[:246]])\n",
        "\n",
        "#Feature ranking solo sul training set per evitare leakage di informazioni\n",
        "tr_outerPhi = np.array(tr_df[outerPhi_imp])\n",
        "tes_outerPhi = np.array(tes_df[outerPhi_imp])\n",
        "val_outerPhi = np.array(val_df[outerPhi_imp])\n",
        "\n",
        "print(f'tr_outerPhi shape: {tr_outerPhi.shape}')\n",
        "print(f'tes_outerPhi shape: {tes_outerPhi.shape}')\n",
        "print(f'val_outerPhi shape: {val_outerPhi.shape}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "transformer = StandardScaler()\n",
        "tr_outerPhi = transformer.fit_transform(tr_outerPhi)\n",
        "tes_outerPhi = transformer.transform(tes_outerPhi)\n",
        "val_outerPhi = transformer.transform(val_outerPhi)\n",
        "\n",
        "transformer = StandardScaler()\n",
        "tr_innerPhi = transformer.fit_transform(tr_innerPhi)\n",
        "tes_innerPhi = transformer.transform(tes_innerPhi)\n",
        "val_innerPhi = transformer.transform(val_innerPhi)\n",
        "\n",
        "transformer = StandardScaler()\n",
        "tr_outerZ = transformer.fit_transform(tr_outerZ)\n",
        "tes_outerZ = transformer.transform(tes_outerZ)\n",
        "val_outerZ = transformer.transform(val_outerZ)\n",
        "\n",
        "transformer = StandardScaler()\n",
        "tr_innerZ = transformer.fit_transform(tr_innerZ)\n",
        "tes_innerZ = transformer.transform(tes_innerZ)\n",
        "val_innerZ = transformer.transform(val_innerZ)\n"
      ],
      "metadata": {
        "id": "-_FXcTb5GrpN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZQpFWYY79Ux3"
      },
      "outputs": [],
      "source": [
        "# OBIETTIVO: RIDURRE LA DIMENSIONE DEL CAMPIONE DI OUTER PHI TRAMITE INTERPOLAZIONE LINEARE\n",
        "#from sklearn.decomposition import PCA\n",
        "#n_comp = tr_innerPhi[0].shape[0]\n",
        "#pca_phi = PCA(n_components=n_comp, random_state=None)\n",
        "#tr_outerPhi = pca_phi.fit_transform(tr_outerPhi)\n",
        "#tes_outerPhi = pca_phi.fit_transform(tes_outerPhi)\n",
        "#val_outerPhi = pca_phi.fit_transform(val_outerPhi)\n",
        "\n",
        "#print(f'tr_outerPhi shape: {tr_outerPhi.shape}')\n",
        "#print(f'tes_outerPhi shape: {tes_outerPhi.shape}')\n",
        "#print(f'val_outerPhi shape: {val_outerPhi.shape}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HtlhREZuXQeU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "140e9ada-ce54-4a45-d776-9b1a76315d47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(197178, 2, 447)\n",
            "(58830, 2, 447)\n",
            "(58982, 2, 447)\n"
          ]
        }
      ],
      "source": [
        "tr_Z = []\n",
        "tes_Z = []\n",
        "val_Z = []\n",
        "\n",
        "for i in range(tr_innerZ.shape[0]):\n",
        "  tr_Z.append(np.vstack([tr_innerZ[i], tr_outerZ[i]]))\n",
        "tr_Z = np.array(tr_Z)\n",
        "\n",
        "for i in range(tes_innerZ.shape[0]):\n",
        "  tes_Z.append(np.vstack([tes_innerZ[i], tes_outerZ[i]]))\n",
        "tes_Z = np.array(tes_Z)\n",
        "\n",
        "for i in range(val_innerZ.shape[0]):\n",
        "  val_Z.append(np.vstack([val_innerZ[i], val_outerZ[i]]))\n",
        "val_Z = np.array(val_Z)\n",
        "\n",
        "\n",
        "\n",
        "print(tr_Z.shape)\n",
        "print(tes_Z.shape)\n",
        "print(val_Z.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aL3Ah7LRsPR2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "881728a4-1a8f-4bdc-e01c-db2839b59c39"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(197178, 2, 246)\n",
            "(58830, 2, 246)\n",
            "(58982, 2, 246)\n"
          ]
        }
      ],
      "source": [
        "tr_Phi = []\n",
        "tes_Phi = []\n",
        "val_Phi = []\n",
        "\n",
        "for i in range(tr_outerPhi.shape[0]):\n",
        "  tr_Phi.append(np.vstack([tr_innerPhi[i], tr_outerPhi[i]]))\n",
        "tr_Phi = np.array(tr_Phi)\n",
        "\n",
        "for i in range(tes_outerPhi.shape[0]):\n",
        "  tes_Phi.append(np.vstack([tes_innerPhi[i], tes_outerPhi[i]]))\n",
        "tes_Phi = np.array(tes_Phi)\n",
        "\n",
        "for i in range(val_outerPhi.shape[0]):\n",
        "  val_Phi.append(np.vstack([val_innerPhi[i], val_outerPhi[i]]))\n",
        "val_Phi = np.array(val_Phi)\n",
        "\n",
        "\n",
        "print(tr_Phi.shape)\n",
        "print(tes_Phi.shape)\n",
        "print(val_Phi.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F0qC_Ucivj_k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f87cc930-8c3e-4401-ec04-7d28384f8247"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(197178, 2, 447)\n",
            "(197178, 2, 246)\n",
            "(197178,)\n",
            "\n",
            "\n",
            "(58830, 2, 447)\n",
            "(58830, 2, 246)\n",
            "(58830,)\n",
            "\n",
            "\n",
            "(58982, 2, 447)\n",
            "(58982, 2, 246)\n",
            "(58982,)\n"
          ]
        }
      ],
      "source": [
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "\n",
        "tr_target = np.array(tr_df[c0_label])\n",
        "tr_Z = tr_Z.astype('float32')\n",
        "tr_Phi = tr_Phi.astype('float32')\n",
        "tr_target = tr_target.astype('int')\n",
        "\n",
        "tes_target = np.array(tes_df[c0_label])\n",
        "tes_Z = tes_Z.astype('float32')\n",
        "tes_Phi = tes_Phi.astype('float32')\n",
        "tes_target = tes_target.astype('int')\n",
        "\n",
        "val_target = np.array(val_df[c0_label])\n",
        "val_Z = val_Z.astype('float32')\n",
        "val_Phi = val_Phi.astype('float32')\n",
        "val_target = val_target.astype('int')\n",
        "\n",
        "print(tr_Z.shape)\n",
        "print(tr_Phi.shape)\n",
        "print(tr_target.shape)\n",
        "print('\\n')\n",
        "\n",
        "print(tes_Z.shape)\n",
        "print(tes_Phi.shape)\n",
        "print(tes_target.shape)\n",
        "print('\\n')\n",
        "\n",
        "print(val_Z.shape)\n",
        "print(val_Phi.shape)\n",
        "print(val_target.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vEWE1pkABE_2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c387cf5-1453-4874-98b5-00ee9f99917d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Z features shape: torch.Size([100, 2, 447])\n",
            "Target shape: torch.Size([100])\n",
            "Phi features shape: torch.Size([100, 2, 246])\n"
          ]
        }
      ],
      "source": [
        "tr_Z = torch.tensor(tr_Z)\n",
        "tr_Phi = torch.tensor(tr_Phi)\n",
        "tr_target = torch.tensor(tr_target)\n",
        "\n",
        "tes_Z = torch.tensor(tes_Z)\n",
        "tes_Phi = torch.tensor(tes_Phi)\n",
        "tes_target = torch.tensor(tes_target)\n",
        "\n",
        "val_Z = torch.tensor(val_Z)\n",
        "val_Phi = torch.tensor(val_Phi)\n",
        "val_target = torch.tensor(val_target)\n",
        "\n",
        "\n",
        "#CREATE torch datasets \n",
        "from torch.utils.data import TensorDataset\n",
        "\n",
        "tr_Dataset = TensorDataset(tr_Phi,tr_Z,tr_target)\n",
        "tes_Dataset = TensorDataset(tes_Phi,tes_Z,tes_target)\n",
        "val_Dataset = TensorDataset(val_Phi,val_Z,val_target)\n",
        "\n",
        "#CREATE DataLoaders\n",
        "from torch.utils.data import DataLoader\n",
        "torch.manual_seed(101)\n",
        "\n",
        "batch_size =  100\n",
        "\n",
        "tr_dataloader = DataLoader(\n",
        "    tr_Dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    drop_last=True\n",
        ")\n",
        "\n",
        "tes_dataloader = DataLoader(\n",
        "    tes_Dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    drop_last=True\n",
        ")\n",
        "\n",
        "val_dataloader = DataLoader(\n",
        "    val_Dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    drop_last=True\n",
        ")\n",
        "\n",
        "\n",
        "u, x, y = next(iter(tr_dataloader))\n",
        "print(\"Z features shape:\", x.shape)\n",
        "print(\"Target shape:\", y.shape)\n",
        "print(\"Phi features shape:\", u.shape)\n",
        "\n",
        "#print(x)\n",
        "#print(u)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mxI79NgzNgas"
      },
      "outputs": [],
      "source": [
        "from numpy.ma.core import outer\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "\n",
        "class myDNN(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(myDNN, self).__init__()\n",
        "\n",
        "    # layers definition\n",
        "\n",
        "    # first convolutional block\n",
        "    self.path1_conv1 = nn.Conv1d(in_channels=2, out_channels=8, kernel_size=7)\n",
        "    self.path1_pool1 = nn.MaxPool1d(kernel_size = 2, stride=2)\n",
        "\n",
        "    # second convolutional block\n",
        "    self.path1_conv2 = nn.Conv1d(in_channels=8, out_channels=16, kernel_size=3)\n",
        "    self.path1_pool2 = nn.MaxPool1d(kernel_size = 2, stride=2)\n",
        "\n",
        "    # third convolutional block\n",
        "    self.path1_conv3 = nn.Conv1d(in_channels=16, out_channels=32, kernel_size=3)\n",
        "    self.path1_pool3 = nn.MaxPool1d(kernel_size = 2, stride=2)\n",
        "\n",
        "    # fourth convolutional block\n",
        "    self.path1_conv4 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=3)\n",
        "    self.path1_pool4 = nn.MaxPool1d(kernel_size = 2, stride=2)\n",
        "\n",
        "    # fifth convolutional block\n",
        "    self.path1_conv5 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3)\n",
        "    self.path1_pool5 = nn.MaxPool1d(kernel_size = 2, stride=2)\n",
        "\n",
        "    self.path2_conv1 = nn.Conv1d(in_channels=2, out_channels=8, kernel_size=7)\n",
        "    self.path2_pool1 = nn.MaxPool1d(kernel_size = 2, stride=2)\n",
        "\n",
        "    # second convolutional block\n",
        "    self.path2_conv2 = nn.Conv1d(in_channels=8, out_channels=16, kernel_size=3)\n",
        "    self.path2_pool2 = nn.MaxPool1d(kernel_size = 2, stride=2)\n",
        "\n",
        "    # third convolutional block\n",
        "    self.path2_conv3 = nn.Conv1d(in_channels=16, out_channels=32, kernel_size=3)\n",
        "    self.path2_pool3 = nn.MaxPool1d(kernel_size = 2, stride=2)\n",
        "\n",
        "    # fourth convolutional block\n",
        "    self.path2_conv4 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=3)\n",
        "    self.path2_pool4 = nn.MaxPool1d(kernel_size = 2, stride=2)\n",
        "\n",
        "    # fifth convolutional block\n",
        "    self.path2_conv5 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3)\n",
        "    self.path2_pool5 = nn.MaxPool1d(kernel_size = 2, stride=2)\n",
        "\n",
        "\n",
        "    self.flatten = nn.Flatten()\n",
        "    self.drop1 = nn.Dropout(p=0.5)\n",
        "    self.fc1 = nn.Linear(in_features=2048, out_features=50) \n",
        "    self.drop2 = nn.Dropout(p=0.5) #dropout\n",
        "    self.fc2 = nn.Linear(in_features=50, out_features=25)\n",
        "    self.fc3 = nn.Linear(in_features=25, out_features=2)\n",
        "\n",
        "\n",
        "  def forward(self, x, y):\n",
        "    x = F.relu(self.path1_conv1(x))\n",
        "    x = self.path1_pool1(x)\n",
        "    x = F.relu(self.path1_conv2(x))\n",
        "    x = self.path1_pool2(x)\n",
        "    x = F.relu(self.path1_conv3(x))\n",
        "    x = self.path1_pool3(x)\n",
        "    x = F.relu(self.path1_conv4(x))\n",
        "    x = self.path1_pool3(x)\n",
        "    x = F.relu(self.path1_conv5(x))\n",
        "    x = self.path1_pool5(x)\n",
        "\n",
        "    y = F.relu(self.path2_conv1(y))\n",
        "    y = self.path2_pool1(y)\n",
        "    y = F.relu(self.path2_conv2(y))\n",
        "    y = self.path2_pool2(y)\n",
        "    y = F.relu(self.path2_conv3(y))\n",
        "    y = self.path2_pool3(y)\n",
        "    y = F.relu(self.path2_conv4(y))\n",
        "    y = self.path2_pool3(y)\n",
        "    y = F.relu(self.path2_conv5(y))\n",
        "    y = self.path2_pool5(y)\n",
        "\n",
        "    #flatten\n",
        "    x = self.flatten(x)\n",
        "    y = self.flatten(y)\n",
        "\n",
        "    w = torch.cat([x,y],dim=1)\n",
        "    w = self.drop1(w) #dropout layer\n",
        "    w = F.relu(self.fc1(w)) #layer fully connected with re lu\n",
        "    w = self.drop2(w)\n",
        "    w = F.relu(self.fc2(w)) #layer fully connected with re lu\n",
        "\n",
        "    w = self.fc3(w) #layer fully connected\n",
        "    #out = F.log_softmax(w, dim=1) non serve, crossentropy lo fa già lui\n",
        "\n",
        "    return w"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "77jDV72enXzf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "070dc88f-3e34-43b4-b84c-1e18d60b826f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "myDNN(\n",
            "  (path1_conv1): Conv1d(2, 8, kernel_size=(7,), stride=(1,))\n",
            "  (path1_pool1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (path1_conv2): Conv1d(8, 16, kernel_size=(3,), stride=(1,))\n",
            "  (path1_pool2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (path1_conv3): Conv1d(16, 32, kernel_size=(3,), stride=(1,))\n",
            "  (path1_pool3): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (path1_conv4): Conv1d(32, 64, kernel_size=(3,), stride=(1,))\n",
            "  (path1_pool4): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (path1_conv5): Conv1d(64, 128, kernel_size=(3,), stride=(1,))\n",
            "  (path1_pool5): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (path2_conv1): Conv1d(2, 8, kernel_size=(7,), stride=(1,))\n",
            "  (path2_pool1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (path2_conv2): Conv1d(8, 16, kernel_size=(3,), stride=(1,))\n",
            "  (path2_pool2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (path2_conv3): Conv1d(16, 32, kernel_size=(3,), stride=(1,))\n",
            "  (path2_pool3): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (path2_conv4): Conv1d(32, 64, kernel_size=(3,), stride=(1,))\n",
            "  (path2_pool4): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (path2_conv5): Conv1d(64, 128, kernel_size=(3,), stride=(1,))\n",
            "  (path2_pool5): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (drop1): Dropout(p=0.5, inplace=False)\n",
            "  (fc1): Linear(in_features=2048, out_features=50, bias=True)\n",
            "  (drop2): Dropout(p=0.5, inplace=False)\n",
            "  (fc2): Linear(in_features=50, out_features=25, bias=True)\n",
            "  (fc3): Linear(in_features=25, out_features=2, bias=True)\n",
            ")\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv1d-1               [-1, 8, 240]             120\n",
            "         MaxPool1d-2               [-1, 8, 120]               0\n",
            "            Conv1d-3              [-1, 16, 118]             400\n",
            "         MaxPool1d-4               [-1, 16, 59]               0\n",
            "            Conv1d-5               [-1, 32, 57]           1,568\n",
            "         MaxPool1d-6               [-1, 32, 28]               0\n",
            "            Conv1d-7               [-1, 64, 26]           6,208\n",
            "         MaxPool1d-8               [-1, 64, 13]               0\n",
            "            Conv1d-9              [-1, 128, 11]          24,704\n",
            "        MaxPool1d-10               [-1, 128, 5]               0\n",
            "           Conv1d-11               [-1, 8, 441]             120\n",
            "        MaxPool1d-12               [-1, 8, 220]               0\n",
            "           Conv1d-13              [-1, 16, 218]             400\n",
            "        MaxPool1d-14              [-1, 16, 109]               0\n",
            "           Conv1d-15              [-1, 32, 107]           1,568\n",
            "        MaxPool1d-16               [-1, 32, 53]               0\n",
            "           Conv1d-17               [-1, 64, 51]           6,208\n",
            "        MaxPool1d-18               [-1, 64, 25]               0\n",
            "           Conv1d-19              [-1, 128, 23]          24,704\n",
            "        MaxPool1d-20              [-1, 128, 11]               0\n",
            "          Flatten-21                  [-1, 640]               0\n",
            "          Flatten-22                 [-1, 1408]               0\n",
            "          Dropout-23                 [-1, 2048]               0\n",
            "           Linear-24                   [-1, 50]         102,450\n",
            "          Dropout-25                   [-1, 50]               0\n",
            "           Linear-26                   [-1, 25]           1,275\n",
            "           Linear-27                    [-1, 2]              52\n",
            "================================================================\n",
            "Total params: 169,777\n",
            "Trainable params: 169,777\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 1.68\n",
            "Forward/backward pass size (MB): 0.32\n",
            "Params size (MB): 0.65\n",
            "Estimated Total Size (MB): 2.65\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "model = myDNN()\n",
        "print(model)\n",
        "from torchsummary import summary\n",
        "if torch.cuda.is_available():\n",
        "  summary(model.cuda(), input_size = [(2,246),(2,447)])\n",
        "else:\n",
        "  summary(model, input_size = [(2,246),(2,447)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sAhpfPaiuR3M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a81cb3d-58f8-45f4-ddf2-dec880182bea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([100, 2])\n",
            "tensor([-0.0400, -0.2173], device='cuda:0', grad_fn=<SelectBackward0>)\n"
          ]
        }
      ],
      "source": [
        "# testing a batch on the model to see if things work\n",
        "\n",
        "feat_phi, feat_z, label = next(iter(tr_dataloader))\n",
        "feat_z=feat_z.to(device) \n",
        "feat_phi=feat_phi.to(device)\n",
        "label=label.to(device)\n",
        "\n",
        "out = model(feat_z,feat_phi) \n",
        "\n",
        "print(out.shape) \n",
        "print(out[0]) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ql_YPByAOoza"
      },
      "outputs": [],
      "source": [
        "# loss function \n",
        "loss_func = nn.CrossEntropyLoss().cuda() #cross entropy loss\n",
        "# 2 classes -> binary cross entropy\n",
        "\n",
        "# metric accuracy (not built in in pytorch)\n",
        "def accuracy(yhat, y):\n",
        "    preds = torch.max(yhat,1)[1] #predictions == neurons with maximum probability \n",
        "    batch_acc = (preds == y).sum()\n",
        "    return batch_acc\n",
        "\n",
        "metric_func = accuracy\n",
        "\n",
        "from torch import optim\n",
        "\n",
        "opt = optim.Adam(model.parameters(), lr=1e-4, betas=(0.9, 0.999), eps = 1e-8, weight_decay=0) \n",
        "scheduler = optim.lr_scheduler.StepLR(opt, 1, gamma=0.99, last_epoch=-1, verbose=False) #ogni 1 epoche il LR è moltiplicato per gamma"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# controlliamo se la GPU è disponibile e nel caso quale tipo di GPU\n",
        "if torch.cuda.is_available(): # True se ho la GPU\n",
        "  print('Numero di GPU disponibili: ',torch.cuda.device_count())\n",
        "  !nvidia-smi\n",
        "  for i in range(0,torch.cuda.device_count()):\n",
        "    print(torch.cuda.get_device_name(0))\n",
        "  \n",
        "\n",
        "# se la GPU è disponibile setto device='cuda', altrimenti 'cpu\n",
        "device = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Computation device: {device}\\n\")"
      ],
      "metadata": {
        "id": "p62VZy-DI8tY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1gtQHMZUOraS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36408520-95e3-4b9b-8873-8978157e669c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ],
      "source": [
        "# model on GPU\n",
        "model.to(device)\n",
        "print(next(model.parameters()).device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gYiZ_CvwOtYz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8da7cbf-ef16-465a-d7cd-42590ae65830"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 1, time(s): 20.7488, train loss: 0.639498, train metric: 0.631720, vali loss: 0.632532, vali metric: 0.640000\n",
            "epoch: 2, time(s): 21.8784, train loss: 0.568751, train metric: 0.706961, vali loss: 0.596349, vali metric: 0.690000\n",
            "epoch: 3, time(s): 20.4080, train loss: 0.544166, train metric: 0.724384, vali loss: 0.565313, vali metric: 0.710000\n",
            "epoch: 4, time(s): 19.6999, train loss: 0.531818, train metric: 0.732796, vali loss: 0.448046, vali metric: 0.820000\n",
            "epoch: 5, time(s): 19.5034, train loss: 0.523045, train metric: 0.738265, vali loss: 0.502660, vali metric: 0.730000\n",
            "epoch: 6, time(s): 19.5843, train loss: 0.516938, train metric: 0.742197, vali loss: 0.478394, vali metric: 0.800000\n",
            "epoch: 7, time(s): 19.9673, train loss: 0.512275, train metric: 0.744937, vali loss: 0.555518, vali metric: 0.700000\n",
            "epoch: 8, time(s): 20.6271, train loss: 0.508898, train metric: 0.747818, vali loss: 0.548215, vali metric: 0.770000\n",
            "epoch: 9, time(s): 19.7401, train loss: 0.506469, train metric: 0.749021, vali loss: 0.536339, vali metric: 0.740000\n",
            "epoch: 10, time(s): 19.5238, train loss: 0.503767, train metric: 0.750309, vali loss: 0.510035, vali metric: 0.770000\n",
            "epoch: 11, time(s): 19.4458, train loss: 0.501506, train metric: 0.751735, vali loss: 0.595901, vali metric: 0.670000\n",
            "epoch: 12, time(s): 19.6122, train loss: 0.500038, train metric: 0.753044, vali loss: 0.512362, vali metric: 0.730000\n",
            "epoch: 13, time(s): 19.6114, train loss: 0.498031, train metric: 0.754749, vali loss: 0.513357, vali metric: 0.730000\n",
            "epoch: 14, time(s): 19.4543, train loss: 0.496421, train metric: 0.755576, vali loss: 0.472046, vali metric: 0.700000\n",
            "epoch: 15, time(s): 19.5912, train loss: 0.493997, train metric: 0.755865, vali loss: 0.541153, vali metric: 0.730000\n",
            "epoch: 16, time(s): 19.4094, train loss: 0.493077, train metric: 0.756707, vali loss: 0.489053, vali metric: 0.790000\n",
            "epoch: 17, time(s): 19.4867, train loss: 0.491382, train metric: 0.758275, vali loss: 0.426768, vali metric: 0.810000\n",
            "epoch: 18, time(s): 19.4595, train loss: 0.490793, train metric: 0.759340, vali loss: 0.559357, vali metric: 0.690000\n",
            "epoch: 19, time(s): 19.8924, train loss: 0.489286, train metric: 0.760457, vali loss: 0.576267, vali metric: 0.660000\n",
            "epoch: 20, time(s): 19.3992, train loss: 0.488044, train metric: 0.760802, vali loss: 0.554743, vali metric: 0.730000\n",
            "epoch: 21, time(s): 19.2602, train loss: 0.486988, train metric: 0.760020, vali loss: 0.531823, vali metric: 0.730000\n",
            "epoch: 22, time(s): 19.2314, train loss: 0.486640, train metric: 0.761228, vali loss: 0.551624, vali metric: 0.710000\n",
            "epoch: 23, time(s): 19.4138, train loss: 0.485965, train metric: 0.762248, vali loss: 0.450598, vali metric: 0.830000\n",
            "epoch: 24, time(s): 19.4436, train loss: 0.485341, train metric: 0.763125, vali loss: 0.445747, vali metric: 0.780000\n",
            "epoch: 25, time(s): 19.3442, train loss: 0.483500, train metric: 0.763242, vali loss: 0.511669, vali metric: 0.740000\n",
            "epoch: 26, time(s): 19.2462, train loss: 0.482888, train metric: 0.763551, vali loss: 0.387188, vali metric: 0.780000\n",
            "epoch: 27, time(s): 19.2348, train loss: 0.480826, train metric: 0.764789, vali loss: 0.496214, vali metric: 0.760000\n",
            "epoch: 28, time(s): 19.3024, train loss: 0.480641, train metric: 0.764926, vali loss: 0.402882, vali metric: 0.820000\n",
            "epoch: 29, time(s): 19.4532, train loss: 0.480131, train metric: 0.765393, vali loss: 0.495242, vali metric: 0.740000\n",
            "epoch: 30, time(s): 19.3489, train loss: 0.479143, train metric: 0.766093, vali loss: 0.405568, vali metric: 0.780000\n",
            "epoch: 31, time(s): 19.2183, train loss: 0.478248, train metric: 0.765611, vali loss: 0.448673, vali metric: 0.760000\n",
            "epoch: 32, time(s): 19.3114, train loss: 0.477628, train metric: 0.767818, vali loss: 0.461796, vali metric: 0.790000\n",
            "epoch: 33, time(s): 19.2915, train loss: 0.476779, train metric: 0.767692, vali loss: 0.416814, vali metric: 0.810000\n",
            "epoch: 34, time(s): 19.2847, train loss: 0.476445, train metric: 0.768026, vali loss: 0.444841, vali metric: 0.780000\n",
            "epoch: 35, time(s): 19.1486, train loss: 0.475768, train metric: 0.768767, vali loss: 0.554875, vali metric: 0.650000\n",
            "epoch: 36, time(s): 19.6675, train loss: 0.474994, train metric: 0.768508, vali loss: 0.480228, vali metric: 0.770000\n",
            "epoch: 37, time(s): 19.3783, train loss: 0.474612, train metric: 0.768737, vali loss: 0.560136, vali metric: 0.710000\n",
            "epoch: 38, time(s): 19.3218, train loss: 0.473558, train metric: 0.769985, vali loss: 0.436755, vali metric: 0.790000\n",
            "epoch: 39, time(s): 19.3441, train loss: 0.473383, train metric: 0.770132, vali loss: 0.398307, vali metric: 0.810000\n",
            "epoch: 40, time(s): 19.3344, train loss: 0.472327, train metric: 0.769655, vali loss: 0.451850, vali metric: 0.770000\n",
            "epoch: 41, time(s): 19.3736, train loss: 0.471242, train metric: 0.771375, vali loss: 0.458115, vali metric: 0.750000\n",
            "epoch: 42, time(s): 19.2647, train loss: 0.470629, train metric: 0.772669, vali loss: 0.457874, vali metric: 0.780000\n",
            "epoch: 43, time(s): 20.6951, train loss: 0.470643, train metric: 0.771857, vali loss: 0.421620, vali metric: 0.840000\n",
            "epoch: 44, time(s): 19.3767, train loss: 0.470040, train metric: 0.772719, vali loss: 0.401003, vali metric: 0.800000\n",
            "epoch: 45, time(s): 19.4184, train loss: 0.470085, train metric: 0.772785, vali loss: 0.450232, vali metric: 0.750000\n",
            "epoch: 46, time(s): 23.8961, train loss: 0.468345, train metric: 0.773283, vali loss: 0.318699, vali metric: 0.850000\n",
            "epoch: 47, time(s): 20.1850, train loss: 0.468096, train metric: 0.773678, vali loss: 0.404923, vali metric: 0.840000\n",
            "epoch: 48, time(s): 23.1836, train loss: 0.467834, train metric: 0.774247, vali loss: 0.376823, vali metric: 0.820000\n",
            "epoch: 49, time(s): 22.5688, train loss: 0.466193, train metric: 0.774906, vali loss: 0.361733, vali metric: 0.800000\n",
            "epoch: 50, time(s): 19.2421, train loss: 0.467119, train metric: 0.773491, vali loss: 0.581227, vali metric: 0.690000\n",
            "epoch: 51, time(s): 19.2666, train loss: 0.466656, train metric: 0.774470, vali loss: 0.560024, vali metric: 0.760000\n",
            "epoch: 52, time(s): 19.2849, train loss: 0.465890, train metric: 0.774323, vali loss: 0.469314, vali metric: 0.750000\n",
            "epoch: 53, time(s): 19.8365, train loss: 0.464738, train metric: 0.775723, vali loss: 0.503994, vali metric: 0.730000\n",
            "epoch: 54, time(s): 19.2725, train loss: 0.465175, train metric: 0.776281, vali loss: 0.445017, vali metric: 0.810000\n",
            "epoch: 55, time(s): 19.1883, train loss: 0.463946, train metric: 0.776454, vali loss: 0.483968, vali metric: 0.780000\n",
            "epoch: 56, time(s): 19.3645, train loss: 0.463191, train metric: 0.776438, vali loss: 0.531320, vali metric: 0.740000\n",
            "epoch: 57, time(s): 19.3274, train loss: 0.462439, train metric: 0.777265, vali loss: 0.441915, vali metric: 0.840000\n",
            "epoch: 58, time(s): 19.5009, train loss: 0.462332, train metric: 0.776657, vali loss: 0.472309, vali metric: 0.740000\n",
            "epoch: 59, time(s): 19.2492, train loss: 0.462351, train metric: 0.776641, vali loss: 0.467890, vali metric: 0.780000\n",
            "epoch: 60, time(s): 19.3264, train loss: 0.461317, train metric: 0.777555, vali loss: 0.544594, vali metric: 0.680000\n",
            "epoch: 61, time(s): 19.3625, train loss: 0.460898, train metric: 0.778184, vali loss: 0.511281, vali metric: 0.750000\n",
            "epoch: 62, time(s): 19.3587, train loss: 0.460907, train metric: 0.778376, vali loss: 0.490928, vali metric: 0.750000\n",
            "epoch: 63, time(s): 19.3389, train loss: 0.460314, train metric: 0.779254, vali loss: 0.665164, vali metric: 0.640000\n",
            "epoch: 64, time(s): 19.3718, train loss: 0.459602, train metric: 0.779239, vali loss: 0.381104, vali metric: 0.850000\n",
            "epoch: 65, time(s): 19.3067, train loss: 0.459948, train metric: 0.778645, vali loss: 0.506089, vali metric: 0.770000\n",
            "epoch: 66, time(s): 19.4371, train loss: 0.459692, train metric: 0.778361, vali loss: 0.371843, vali metric: 0.830000\n",
            "epoch: 67, time(s): 19.3023, train loss: 0.459038, train metric: 0.779006, vali loss: 0.556468, vali metric: 0.720000\n",
            "epoch: 68, time(s): 19.1877, train loss: 0.458841, train metric: 0.779320, vali loss: 0.458570, vali metric: 0.770000\n",
            "epoch: 69, time(s): 19.2616, train loss: 0.458658, train metric: 0.779990, vali loss: 0.486106, vali metric: 0.740000\n",
            "epoch: 70, time(s): 19.9935, train loss: 0.457562, train metric: 0.780660, vali loss: 0.432846, vali metric: 0.820000\n",
            "epoch: 71, time(s): 19.3749, train loss: 0.456218, train metric: 0.781055, vali loss: 0.425938, vali metric: 0.820000\n",
            "epoch: 72, time(s): 19.4391, train loss: 0.457589, train metric: 0.779904, vali loss: 0.523314, vali metric: 0.790000\n",
            "epoch: 73, time(s): 19.2203, train loss: 0.457083, train metric: 0.779822, vali loss: 0.571155, vali metric: 0.700000\n",
            "epoch: 74, time(s): 19.3529, train loss: 0.456138, train metric: 0.780913, vali loss: 0.529064, vali metric: 0.740000\n",
            "epoch: 75, time(s): 19.2860, train loss: 0.456062, train metric: 0.780979, vali loss: 0.455388, vali metric: 0.820000\n",
            "epoch: 76, time(s): 19.3295, train loss: 0.454611, train metric: 0.782562, vali loss: 0.455284, vali metric: 0.750000\n",
            "epoch: 77, time(s): 19.3119, train loss: 0.454402, train metric: 0.781806, vali loss: 0.516056, vali metric: 0.790000\n",
            "epoch: 78, time(s): 19.3130, train loss: 0.454250, train metric: 0.782461, vali loss: 0.316170, vali metric: 0.930000\n",
            "epoch: 79, time(s): 19.3131, train loss: 0.453624, train metric: 0.782070, vali loss: 0.435167, vali metric: 0.790000\n",
            "epoch: 80, time(s): 19.2147, train loss: 0.453185, train metric: 0.782552, vali loss: 0.432902, vali metric: 0.810000\n",
            "epoch: 81, time(s): 19.3161, train loss: 0.453592, train metric: 0.781791, vali loss: 0.392207, vali metric: 0.800000\n",
            "epoch: 82, time(s): 19.2376, train loss: 0.452629, train metric: 0.783496, vali loss: 0.442373, vali metric: 0.770000\n",
            "epoch: 83, time(s): 19.1946, train loss: 0.451985, train metric: 0.783998, vali loss: 0.582422, vali metric: 0.690000\n",
            "epoch: 84, time(s): 19.2550, train loss: 0.451827, train metric: 0.783643, vali loss: 0.486041, vali metric: 0.720000\n",
            "epoch: 85, time(s): 19.3497, train loss: 0.452088, train metric: 0.783237, vali loss: 0.368155, vali metric: 0.820000\n"
          ]
        }
      ],
      "source": [
        "# in pytorch è necessario scriversi il loop di training, cioè il loop sulle epoch di training in cui in ogni \n",
        "# epoca si leggono tutti gli eventi del dataset aggiornando i pesi dopo ogni mini-batch\n",
        "\n",
        "\n",
        "# numero di epoche\n",
        "epochs = 100\n",
        "\n",
        "# liste su cui salvare il valore della loss e della metrica ad ogni epoca per poterli graficare in funzione \n",
        "# dell'epoca a fine addestramento\n",
        "hist_loss = []\n",
        "hist_metric = []\n",
        "hist_vloss = []\n",
        "hist_vmetric = []\n",
        "\n",
        "\n",
        "\n",
        "# loop sulle epoche\n",
        "for epoch in range(epochs):\n",
        "    t0 = time.time()\n",
        "    \n",
        "    # training step (in cui aggiorniamo i pesi della rete neurale)\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    train_metric = 0\n",
        "    counter = 0\n",
        "\n",
        "    for inputs1, inputs2, targets in tr_dataloader:  \n",
        "        counter += 1\n",
        "        inputs1=inputs1.to(device) \n",
        "        inputs2=inputs2.to(device) \n",
        "        targets=targets.to(device)\n",
        "\n",
        "        pred = model(inputs1,inputs2)\n",
        "\n",
        "        # calcolo loss e metrica\n",
        "        loss = loss_func(pred, targets)\n",
        "        with torch.no_grad():\n",
        "          metric = metric_func(pred, targets)\n",
        "\n",
        "        # aggiorno la loss e metrica totale\n",
        "        train_loss += loss.item()\n",
        "        train_metric += metric.item()\n",
        "\n",
        "        # backpropagation\n",
        "        opt.zero_grad() #resetta i gradienti prima di eseguire la backpropagation\n",
        "        loss.backward() #calcola i gradienti della loss\n",
        "        opt.step() #aggiorna i pesi\n",
        "\n",
        "    train_loss /= counter\n",
        "    train_metric /= (counter*batch_size)\n",
        "    hist_loss.append(train_loss)\n",
        "    hist_metric.append(train_metric)\n",
        "\n",
        "\n",
        "    \n",
        "        # validation step (non vengono aggiornati i pesi), si misurano le performance sul validation\n",
        "    model.eval()\n",
        "    vali_loss = 0\n",
        "    vali_metric = 0\n",
        "    counter = 0\n",
        "    with torch.no_grad(): #evita che si aggiornino i pesi\n",
        "      for inputs1, inputs2, targets in val_dataloader:  \n",
        "        counter +=1\n",
        "        inputs1=inputs1.to(device) \n",
        "        inputs2=inputs2.to(device) \n",
        "        targets=targets.to(device)\n",
        "\n",
        "        pred = model(inputs1,inputs2)\n",
        "\n",
        "        # calcolo loss e metrica\n",
        "        vloss = loss_func(pred, targets)\n",
        "        vmetric = metric_func(pred, targets)\n",
        "\n",
        "        vali_loss += loss.item()\n",
        "        vali_metric += metric.item() \n",
        "\n",
        "    vali_loss /= counter\n",
        "    vali_metric /= (counter*batch_size)\n",
        "    hist_vloss.append(vali_loss)\n",
        "    hist_vmetric.append(vali_metric)\n",
        "\n",
        "\n",
        "    elapsed_time = time.time()-t0\n",
        "    print(\"epoch: %d, time(s): %.4f, train loss: %.6f, train metric: %.6f, vali loss: %.6f, vali metric: %.6f\" \n",
        "          % (epoch+1, elapsed_time, train_loss, train_metric, vali_loss, vali_metric))\n",
        "    scheduler.step()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def ewma_vectorized(data, alpha, offset=None, dtype=None, order='C', out=None):\n",
        "    \"\"\"\n",
        "    Calculates the exponential moving average over a vector.\n",
        "    Will fail for large inputs.\n",
        "    :param data: Input data\n",
        "    :param alpha: scalar float in range (0,1)\n",
        "        The alpha parameter for the moving average.\n",
        "    :param offset: optional\n",
        "        The offset for the moving average, scalar. Defaults to data[0].\n",
        "    :param dtype: optional\n",
        "        Data type used for calculations. Defaults to float64 unless\n",
        "        data.dtype is float32, then it will use float32.\n",
        "    :param order: {'C', 'F', 'A'}, optional\n",
        "        Order to use when flattening the data. Defaults to 'C'.\n",
        "    :param out: ndarray, or None, optional\n",
        "        A location into which the result is stored. If provided, it must have\n",
        "        the same shape as the input. If not provided or `None`,\n",
        "        a freshly-allocated array is returned.\n",
        "    \"\"\"\n",
        "    data = np.array(data, copy=False)\n",
        "\n",
        "    if dtype is None:\n",
        "        if data.dtype == np.float32:\n",
        "            dtype = np.float32\n",
        "        else:\n",
        "            dtype = np.float64\n",
        "    else:\n",
        "        dtype = np.dtype(dtype)\n",
        "\n",
        "    if data.ndim > 1:\n",
        "        # flatten input\n",
        "        data = data.reshape(-1, order)\n",
        "\n",
        "    if out is None:\n",
        "        out = np.empty_like(data, dtype=dtype)\n",
        "    else:\n",
        "        assert out.shape == data.shape\n",
        "        assert out.dtype == dtype\n",
        "\n",
        "    if data.size < 1:\n",
        "        # empty input, return empty array\n",
        "        return out\n",
        "\n",
        "    if offset is None:\n",
        "        offset = data[0]\n",
        "\n",
        "    alpha = np.array(alpha, copy=False).astype(dtype, copy=False)\n",
        "\n",
        "    # scaling_factors -> 0 as len(data) gets large\n",
        "    # this leads to divide-by-zeros below\n",
        "    scaling_factors = np.power(1. - alpha, np.arange(data.size + 1, dtype=dtype),\n",
        "                               dtype=dtype)\n",
        "    # create cumulative sum array\n",
        "    np.multiply(data, (alpha * scaling_factors[-2]) / scaling_factors[:-1],\n",
        "                dtype=dtype, out=out)\n",
        "    np.cumsum(out, dtype=dtype, out=out)\n",
        "\n",
        "    # cumsums / scaling\n",
        "    out /= scaling_factors[-2::-1]\n",
        "\n",
        "    if offset != 0:\n",
        "        offset = np.array(offset, copy=False).astype(dtype, copy=False)\n",
        "        # add offsets\n",
        "        out += offset * scaling_factors[1:]\n",
        "\n",
        "    return out"
      ],
      "metadata": {
        "id": "ahPork3wNlC4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vi1VKa0xPrdI"
      },
      "outputs": [],
      "source": [
        "# plot loss vs epoca\n",
        "plt.figure(figsize=(10, 7))\n",
        "ewma_hist_vloss = ewma_vectorized(hist_vloss,0.1)\n",
        "plt.plot(range(1,len(hist_loss)+1), hist_loss, color='green', linestyle='-', label='Train Loss')\n",
        "plt.plot(range(1,len(hist_vloss)+1), ewma_hist_vloss, color='blue', linestyle='-', label='Moving Average Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E0QEBcI3QZLU"
      },
      "outputs": [],
      "source": [
        "# plot accuracy vs epoca\n",
        "plt.figure(figsize=(10, 7))\n",
        "ewma_hist_vmetric = ewma_vectorized(hist_vmetric,0.1)\n",
        "plt.plot(range(1,len(hist_metric)+1),hist_metric, color='green', linestyle='-', label='Train Metric')\n",
        "plt.plot(range(1,len(hist_vmetric)+1),ewma_hist_vmetric, color='blue', linestyle='-', label='Moving Average Validation Metric')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Metric')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ox8LkInhQj3b"
      },
      "outputs": [],
      "source": [
        "# Salvataggio del modello\n",
        "\n",
        "torch.save(model, './trained_model.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BtNzeYz033FY"
      },
      "outputs": [],
      "source": [
        "# Uso di un modello addestrato in inferenza (predizione) \n",
        "\n",
        "# lettura del modello\n",
        "\n",
        "model = torch.load('./trained_model.pt')\n",
        "model.eval() #va sempre fatto per settare eventuali layer di droput e batch norm in evaluation model\n",
        "\n",
        "# Non è strettamento necessario girare sulla GPU per l'inferenza (non richiede grande potenza di calcolo)\n",
        "\n",
        "loss = 0\n",
        "metric = 0\n",
        "counter = 0\n",
        "\n",
        "for inputs1, inputs2, targets in tes_dataloader:\n",
        "  counter +=1\n",
        "  inputs1=inputs1.to(device) \n",
        "  inputs2=inputs2.to(device) \n",
        "  targets=targets.to(device)\n",
        "  pred = model(inputs1,inputs2)\n",
        "\n",
        "  loss += loss_func(pred, targets).item()\n",
        "  metric += accuracy(pred, targets).item()\n",
        "\n",
        "\n",
        "loss /= counter\n",
        "metric /= (counter*batch_size)\n",
        "\n",
        "print('Test loss/accuracy: ',loss, ' / ', metric)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4OPM9IEV5dHw"
      },
      "outputs": [],
      "source": [
        "#confusion matrix\n",
        "\n",
        "model.to(torch.device('cpu'))\n",
        "\n",
        "predictions = np.empty(shape=(0,))\n",
        "truths = np.empty(shape=(0,))\n",
        " \n",
        "for inputs1,inputs2,targets in tes_dataloader:\n",
        "  counter += 1\n",
        "  inputs1=inputs1.to('cpu')\n",
        "  inputs2=inputs2.to('cpu')\n",
        "  targets=targets.to('cpu')\n",
        "  pred = model(inputs1,inputs2)\n",
        "\n",
        "  res = torch.argmax(pred, dim=1)\n",
        "\n",
        "  predictions = np.concatenate((predictions,res.detach().numpy()))\n",
        "  truths = np.concatenate((truths,targets.detach().numpy()))\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "c_mat = confusion_matrix(predictions, truths, normalize='true')\n",
        "print('Matrice di correlazione: ')\n",
        "print(c_mat)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "fpr__0 = []\n",
        "tpr__0 = []\n",
        "roc_auc__0 = []\n",
        "\n",
        "fpr__1 = []\n",
        "tpr__1 = []\n",
        "roc_auc__1 = []\n",
        "\n",
        "for i in range(0,1000):\n",
        "\n",
        "  inputs1,inputs2,targets = next(iter(tes_dataloader))\n",
        "\n",
        "  pred=model(inputs1,inputs2)\n",
        "\n",
        "  targets = targets.detach().numpy()\n",
        "  pred = pred.detach().numpy()\n",
        "\n",
        "  fpr_0, tpr_0, thresholds_0 = metrics.roc_curve(targets, pred[:,0], pos_label=0, drop_intermediate=False)\n",
        "  roc_auc_0 = metrics.auc(fpr_0, tpr_0)\n",
        "\n",
        "  fpr_1, tpr_1, thresholds_1 = metrics.roc_curve(targets, pred[:,1], pos_label=1, drop_intermediate=False)\n",
        "  roc_auc_1 = metrics.auc(fpr_1, tpr_1)\n",
        "\n",
        "  fpr__0.append(fpr_0)\n",
        "  tpr__0.append(tpr_0)\n",
        "  roc_auc__0.append(roc_auc_0)\n",
        "\n",
        "  fpr__1.append(fpr_1)\n",
        "  tpr__1.append(tpr_1)\n",
        "  roc_auc__1.append(roc_auc_1)\n",
        "\n",
        "\n",
        "print(f'label 0: ROC mean = {np.mean(roc_auc__0)}    Standard deviation = {np.std(roc_auc__0)}')\n",
        "print(f'label 1: ROC mean = {np.mean(roc_auc__1)}    Standard deviation = {np.std(roc_auc__1)}')"
      ],
      "metadata": {
        "id": "mPxuD2LFcc6v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fpr_0_mean = [sum(x)/len(x) for x in zip(*fpr__0)]\n",
        "fpr_1_mean = [sum(x)/len(x) for x in zip(*fpr__1)]\n",
        "tpr_0_mean = [sum(x)/len(x) for x in zip(*tpr__0)]\n",
        "tpr_1_mean = [sum(x)/len(x) for x in zip(*tpr__1)]"
      ],
      "metadata": {
        "id": "6vE_433Z3Fng"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure()\n",
        "lw = 2\n",
        "plt.plot(fpr_0_mean, tpr_0_mean, color='sandybrown',\n",
        " lw=lw, label='Average ROC curve label 0 (Average AUC = %0.2f)' % np.mean(roc_auc__0))\n",
        "plt.plot(fpr_1_mean, tpr_1_mean, color='limegreen',\n",
        " lw=lw, label='Average ROC curve label 1 (Average AUC = %0.2f)' % np.mean(roc_auc__1))\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver operating characteristic (ROC)')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9eLNxMi5BRPq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "-rSZq9I4fYGq"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Copy of Machine_Learning_Progetto_7.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}